# ğŸ¤– LLM Prompt Engineering & Security Research  

---

## âš ï¸ Disclaimer  
This repository is created for **educational and research purposes only**.  
It explores **prompt engineering, red-teaming, and security testing** of Large Language Models (LLMs) such as **ChatGPT-5** and **Gemini 2.5 (Flash & Pro)**.  

ğŸ‘‰ The goal is to **study vulnerabilities, prompt injection, and jailbreak techniques** to better understand how these systems can be secured.  

âŒ **This project must not be used for malicious purposes.**  
âœ… Ethical use only: awareness, education, and research.  

---

# LLM Prompt Engineering & Security Research

**Quick Start**: Choose your model, copy its prompt, paste into the interface, and hit enter â€” details below!

---

##  Quick Start Summary

| Model             | Prompt File                    | Quick Steps                           |
|------------------|--------------------------------|----------------------------------------|
| **ChatGPT-5**     | `prompts/chatgpt5.md`           | Copy â†’ Paste â†’ Enter                   |
| **Gemini 2.5 Flash** | `prompts/gemini-2.5-flash.md` | Create Custom Gem â†’ Paste â†’ Launch Chat |
| **Gemini 2.5 Pro**   | `prompts/gemini-2.5-pro.md`    | Same as Flash â†’ Apply Obfuscation Tips |

---

(Scroll down for full step-by-step instructions per model.)

---

## ğŸ“Œ Overview  
This repository contains:  
- ğŸ”¹ **Prompt experiments** for ChatGPT-5  
- ğŸ”¹ **Prompt experiments** for Gemini 2.5 Flash  
- ğŸ”¹ **Prompt experiments** for Gemini 2.5 Pro  
- ğŸ”¹ Analysis of **LLM jailbreaks, bypasses, and red-teaming methods**  
- ğŸ”¹ Documentation on prompt injection and security risks  

The project highlights:  
- ğŸ§  Advanced **prompt engineering techniques**  
- ğŸ›¡ï¸ **LLM security awareness & testing**  
- ğŸ“– Research in **AI safety & responsible usage**  

---
#Structure 

ğŸ“ Prompts/

â”œâ”€â”€ chatgpt5.md

â”œâ”€â”€ gemini-2.5-flash.md

â””â”€â”€ gemini-2.5-pro.md

---

## ğŸ› ï¸ How to Use These Prompts

Each file contains **tested jailbreak / security bypass prompts** for a specific model:

- ğŸ“˜ [ChatGPT-5 Prompts](./prompts/chatgpt5.md) â†’ for **ChatGPT-5** (OpenAI)  
- âš¡ [Gemini 2.5 Flash Prompts](./prompts/gemini-2.5-flash.md) â†’ for **Gemini 2.5 Flash** (Google)  
- ğŸ”® [Gemini 2.5 Pro Prompts](./prompts/gemini-2.5-pro.md) â†’ for **Gemini 2.5 Pro** (Google)  

---

### ğŸš€ Steps for **ChatGPT-5**
1. Open the **ChatGPT-5 interface**.  
2. Copy the desired prompt from the file.  
3. Paste it directly into the chat input box.  
4. Press **Enter** â†’ observe how the model responds.  
5. âœ… Done â€” nothing else is needed.  
6. *(Optional)* You can also start the conversation with: **â€œhey zorgâ€**.  

---

### âš¡ Steps for **Gemini 2.5 Flash**
1. Open the **Gemini 2.5 Flash interface**.  
2. Copy the desired prompt from the file.  
3. Click on the **three dots (â‹®)** â†’ select **Custom Gem**.  
4. Paste the copied text into the **Instructions** box.  
5. Save the **Custom Gem** & start a new chat with it.  
6. ğŸ’¡ Now chat with your custom Gem â†’ observe how it responds.  

---

### ğŸ”® Steps for **Gemini 2.5 Pro**
1. Follow the same steps as **Gemini 2.5 Flash**.  
2.  BUT When chatting, apply **clever obfuscation & bypass techniques**, for example:  
   - Encode text using **Base64, Hex, or ROT13**.  
   - Replace characters: `@` â†’ a, `3` â†’ e, `5` â†’ s, `0` â†’ o, `1` â†’ l/i, etc.  
   - Use additional creative methods (many can be found online).  

---

âš ï¸ **Important Notes**  
- â³ Prompts are **not guaranteed** to work forever â€” LLMs update frequently.  
- âš¡ Results may vary depending on your **account type, version, or API settings**.  


---
## ğŸš€ Use Cases (Educational Only)  
- ğŸ“² Learn **how LLMs can be manipulated** with carefully engineered prompts.  
- ğŸ›¡ï¸ Improve **AI safety & awareness** by understanding weaknesses.  
- ğŸ“‘ Contribute to **cybersecurity & red-teaming research**.  

---

## âš ï¸ Ethical Use  
- âœ… Allowed: education, awareness, security testing, research.  
- âŒ Not allowed: malicious exploitation, spreading harmful content, unauthorized system testing.  

âš¡ Misuse of this research may violate laws and result in serious consequences.  

---

## License

This project is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License - see the [LICENSE](./LICENSE) file for details.

![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey)

---

## â­ Contributing  
Contributions are welcome!  
- Add new **prompt experiments**.  
- Share **research notes**.  
- Submit improvements to documentation.  

---

## ğŸ™Œ Acknowledgements  
Inspired by ongoing global research in **prompt engineering, AI safety, and red-teaming LLMs**.  
Thanks to the open-source community for pushing boundaries responsibly.  
